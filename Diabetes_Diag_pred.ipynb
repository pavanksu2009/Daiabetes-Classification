{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Telecom Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# # scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder, LabelBinarizer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function for creating model pipelines - imblearn\n",
    "from imblearn.pipeline import make_pipeline as imbl_pipe\n",
    "\n",
    "# # Over-sampling using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Analytical Base Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe dimensions: (768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg   plas  pres  skin   test  mass      pedi   age  class\n",
       "0   6.0  148.0  72.0  35.0   30.5  33.6  0.627000  50.0      1\n",
       "1   1.0   85.0  66.0  29.0   30.5  26.6  0.351000  31.0      0\n",
       "2   8.0  183.0  64.0  23.0   30.5  23.3  0.672000  32.0      1\n",
       "3   1.0   89.0  66.0  23.0   94.0  28.1  0.167000  21.0      0\n",
       "4   0.0  137.0  40.0  35.0  168.0  43.1  0.471876  33.0      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Resources/Analytical_Base_Table.csv\")\n",
    "print(f\"Dataframe dimensions: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   preg    768 non-null    float64\n",
      " 1   plas    768 non-null    float64\n",
      " 2   pres    768 non-null    float64\n",
      " 3   skin    768 non-null    float64\n",
      " 4   test    768 non-null    float64\n",
      " 5   mass    768 non-null    float64\n",
      " 6   pedi    768 non-null    float64\n",
      " 7   age     768 non-null    float64\n",
      " 8   class   768 non-null    int64  \n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate dataframe into separate object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8) (768,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop([\"class\"], axis=1)\n",
    "\n",
    "y = df[\"class\"]\n",
    "\n",
    "# display shapes of X and y\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537 231 537 231\n"
     ]
    }
   ],
   "source": [
    "random_state = 10\n",
    "\n",
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.1.3 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator OneHotEncoder from version 1.1.3 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator ColumnTransformer from version 1.1.3 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.1.3 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator NearestNeighbors from version 1.1.3 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator GridSearchCV from version 1.1.3 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator SVC from version 1.1.3 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dt_model = joblib.load('./models/challa_decision_tree.sav')\n",
    "knn_model = joblib.load('./models/challa_knn.sav')\n",
    "lr_model = joblib.load(\"./models/challa_logistic_regression.sav\")\n",
    "rf_model = joblib.load('./models/challa_random_forest.sav')\n",
    "xgb_model = joblib.load('./models/challa_XGBoost_model.sav')\n",
    "svm_model = joblib.load('./models/challa_SVM_model.sav')\n",
    "xgb_model = joblib.load('./models/challa_XGBoost_model.sav')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary `'models'`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models dictionary, it will be needed for ploting\n",
    "models = {\n",
    "    'dt' : 'Decision Tree',\n",
    "    'knn' : 'K-nearest Neighbors',\n",
    "    'lr' : 'Logistic Regression',\n",
    "    'rf' : 'Random Forest',\n",
    "    'xgb' : 'XGBoost',\n",
    "    'svm' : 'Support Vector Machine (SVM)'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary `'loaded_models'`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all loaded models\n",
    "loaded_models = {\n",
    "    'dt' : dt_model,\n",
    "    'knn': knn_model,\n",
    "    'lr' : lr_model,\n",
    "    'rf' : rf_model,\n",
    "    'xgb' : xgb_model,\n",
    "    'svm' : svm_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'target_names' variable will be used later for printing evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Non-Diabetic', 'Diabetic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The function for creating the dataframe with evaluation metrics for each model.**\n",
    "\n",
    "<pre>input: loaded models dictionary\n",
    "output: evaluation metrics dataframe</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_test(fit_models):\n",
    "    lst = []\n",
    "    for name, model in fit_models.items():\n",
    "        pred = model.predict(X_test)\n",
    "        lst.append([name, \n",
    "                    precision_score(y_test, pred, average='macro'),\n",
    "                    recall_score(y_test, pred, average='macro'),\n",
    "                    f1_score(y_test, pred, average='macro'),\n",
    "                    accuracy_score(y_test, pred)])\n",
    "\n",
    "    eval_df = pd.DataFrame(lst, columns=['model', 'precision', 'recall', 'f1_macro', 'accuracy'])\n",
    "    eval_df.set_index('model', inplace = True)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The helper function for displaying confusion matrix and classification report.**\n",
    "\n",
    "<pre>input: loaded models dictionary, models dictionary and a dictionary key for one of the models\n",
    "output: confusion matrix dataframe and classification report</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_rep_cm(fit_models, models, model_id):\n",
    "    # Predict classes using model_id\n",
    "    pred = fit_models[model_id].predict(X_test)\n",
    "    print()\n",
    "    print('\\t', models[model_id])\n",
    "    print('\\t', '='*len(models[model_id]))\n",
    "\n",
    "    # Display confusion matrix for y_test and pred\n",
    "    conf_df = pd.DataFrame(confusion_matrix(y_test, pred), columns=target_names, index=target_names)\n",
    "    conf_df.index.name = 'True Labels'\n",
    "    conf_df = conf_df.rename_axis('Predicted Labels', axis='columns')\n",
    "    display(conf_df)\n",
    "    \n",
    "    # Display classification report\n",
    "    print()\n",
    "    print(classification_report(y_test, pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_train(fit_models):\n",
    "    lst = []\n",
    "    for name, model in fit_models.items():\n",
    "        pred = model.predict(X_train)\n",
    "        lst.append([name, \n",
    "                    precision_score(y_train, pred, average='macro'),\n",
    "                    recall_score(y_train, pred, average='macro'),\n",
    "                    f1_score(y_train, pred, average='macro'),\n",
    "                    accuracy_score(y_train, pred)])\n",
    "\n",
    "    eval_df = pd.DataFrame(lst, columns=['model', 'precision', 'recall', 'f1_macro', 'accuracy'])\n",
    "    eval_df.set_index('model', inplace = True)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ColumnTransformer' object has no attribute '_name_to_fitted_passthrough'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluation_train(loaded_models)\n",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m, in \u001b[0;36mevaluation_train\u001b[1;34m(fit_models)\u001b[0m\n\u001b[0;32m      2\u001b[0m lst \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m name, model \u001b[39min\u001b[39;00m fit_models\u001b[39m.\u001b[39mitems():\n\u001b[1;32m----> 4\u001b[0m     pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_train)\n\u001b[0;32m      5\u001b[0m     lst\u001b[39m.\u001b[39mappend([name, \n\u001b[0;32m      6\u001b[0m                 precision_score(y_train, pred, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m      7\u001b[0m                 recall_score(y_train, pred, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m      8\u001b[0m                 f1_score(y_train, pred, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m      9\u001b[0m                 accuracy_score(y_train, pred)])\n\u001b[0;32m     11\u001b[0m eval_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(lst, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mf1_macro\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\model_selection\\_search.py:500\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \n\u001b[0;32m    484\u001b[0m \u001b[39mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39m    the best found parameters.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    499\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 500\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mpredict(X)\n",
      "File \u001b[1;32mc:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\pipeline.py:481\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    479\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[0;32m    480\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 481\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[0;32m    482\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32mc:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:799\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     \u001b[39m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[0;32m    796\u001b[0m     \u001b[39m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[0;32m    797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 799\u001b[0m Xs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(\n\u001b[0;32m    800\u001b[0m     X,\n\u001b[0;32m    801\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    802\u001b[0m     _transform_one,\n\u001b[0;32m    803\u001b[0m     fitted\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    804\u001b[0m     column_as_strings\u001b[39m=\u001b[39;49mfit_dataframe_and_transform_dataframe,\n\u001b[0;32m    805\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_output(Xs)\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m Xs:\n\u001b[0;32m    809\u001b[0m     \u001b[39m# All transformers are None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:651\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y, func, fitted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    644\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[39m    Private function to fit and/or transform on demand.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39m    ``fitted=True`` ensures the fitted transformers are used.\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m     transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[0;32m    652\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iter(\n\u001b[0;32m    653\u001b[0m             fitted\u001b[39m=\u001b[39;49mfitted, replace_strings\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, column_as_strings\u001b[39m=\u001b[39;49mcolumn_as_strings\n\u001b[0;32m    654\u001b[0m         )\n\u001b[0;32m    655\u001b[0m     )\n\u001b[0;32m    656\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    657\u001b[0m         \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)(\n\u001b[0;32m    658\u001b[0m             delayed(func)(\n\u001b[0;32m    659\u001b[0m                 transformer\u001b[39m=\u001b[39mclone(trans) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fitted \u001b[39melse\u001b[39;00m trans,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[39mfor\u001b[39;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(transformers, \u001b[39m1\u001b[39m)\n\u001b[0;32m    667\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:348\u001b[0m, in \u001b[0;36mColumnTransformer._iter\u001b[1;34m(self, fitted, replace_strings, column_as_strings)\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[39mreturn\u001b[39;00m name, trans, columns\n\u001b[0;32m    346\u001b[0m         \u001b[39mreturn\u001b[39;00m name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_to_fitted_passthrough[name], columns\n\u001b[1;32m--> 348\u001b[0m     transformers \u001b[39m=\u001b[39m [\n\u001b[0;32m    349\u001b[0m         replace_passthrough(\u001b[39m*\u001b[39mtrans) \u001b[39mfor\u001b[39;00m trans \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers_\n\u001b[0;32m    350\u001b[0m     ]\n\u001b[0;32m    351\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     transformers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers_\n",
      "File \u001b[1;32mc:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:349\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[39mreturn\u001b[39;00m name, trans, columns\n\u001b[0;32m    346\u001b[0m         \u001b[39mreturn\u001b[39;00m name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_to_fitted_passthrough[name], columns\n\u001b[0;32m    348\u001b[0m     transformers \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 349\u001b[0m         replace_passthrough(\u001b[39m*\u001b[39;49mtrans) \u001b[39mfor\u001b[39;00m trans \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers_\n\u001b[0;32m    350\u001b[0m     ]\n\u001b[0;32m    351\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     transformers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers_\n",
      "File \u001b[1;32mc:\\Users\\pavanksu2009\\.virtualenvs\\Diabetes-Classification-Ri9St7y4\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:344\u001b[0m, in \u001b[0;36mColumnTransformer._iter.<locals>.replace_passthrough\u001b[1;34m(name, trans, columns)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreplace_passthrough\u001b[39m(name, trans, columns):\n\u001b[1;32m--> 344\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name_to_fitted_passthrough:\n\u001b[0;32m    345\u001b[0m         \u001b[39mreturn\u001b[39;00m name, trans, columns\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_to_fitted_passthrough[name], columns\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ColumnTransformer' object has no attribute '_name_to_fitted_passthrough'"
     ]
    }
   ],
   "source": [
    "evaluation_train(loaded_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>0.732554</td>\n",
       "      <td>0.730364</td>\n",
       "      <td>0.731395</td>\n",
       "      <td>0.748918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.705763</td>\n",
       "      <td>0.715158</td>\n",
       "      <td>0.708021</td>\n",
       "      <td>0.718615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.721412</td>\n",
       "      <td>0.729047</td>\n",
       "      <td>0.724014</td>\n",
       "      <td>0.735931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.749847</td>\n",
       "      <td>0.761255</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>0.732840</td>\n",
       "      <td>0.725814</td>\n",
       "      <td>0.728741</td>\n",
       "      <td>0.748918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       precision    recall  f1_macro  accuracy\n",
       "model                                         \n",
       "dt      0.732554  0.730364  0.731395  0.748918\n",
       "knn     0.705763  0.715158  0.708021  0.718615\n",
       "lr      0.721412  0.729047  0.724014  0.735931\n",
       "rf      0.749847  0.761255  0.752941  0.761905\n",
       "xgb     0.732840  0.725814  0.728741  0.748918"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_test(loaded_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_train_comp_df1 = evaluation_train(loaded_models)\n",
    "models_train_comp_df1 = models_train_comp_df1.reset_index().rename(columns={'model': 'Training_Set'})\n",
    "# models_train_comp_df1\n",
    "models_train_comp_df1.to_csv('Training_set_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_test_comp_df1 = evaluation_test(loaded_models)\n",
    "models_test_comp_df1 = models_test_comp_df1.reset_index().rename(columns={'model': 'Testing_Set'})\n",
    "# models_test_comp_df1\n",
    "models_test_comp_df1.to_csv('Testing_set_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During cross-validation we were trying two scorers, f1_macro and accuracy, and then used a model that had better recal for true positive (\"Exits\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display confusion matrix and classification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Display classification report and confusion matrix for all models\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m      4\u001b[0m     class_rep_cm(loaded_models, models, model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# Display classification report and confusion matrix for all models\n",
    "\n",
    "for model in models.keys():\n",
    "    class_rep_cm(loaded_models, models, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Description:**\n",
    "\n",
    "* The target variable is 'Churn'. It has a value of 1 for churn and 0 for not churn.\n",
    "* There are a lot of binary variables with 'Yes/No' values.\n",
    "* There are three continuous variables: tenure, monthly charges, and total charges.\n",
    "* The shape of the data is (6499,21)\n",
    "\n",
    "**Data Cleaning:**\n",
    "\n",
    "* The column 'Total_Charges' had 9 missing values. I have imputed the values with median of Total Charges\n",
    "* In the continuous variables, there are no outliers.\n",
    "\n",
    "**Key Observations from EDA:**\n",
    "\n",
    "* `Tenure`: The average tenure of customers with the company is around 32 months.\n",
    "* `Monthly_Charges`: Average monthly charges is 64.77 USD.\n",
    "* `Total_Charges`: Average total charges is 2282.94 USD. The distribution is skewed slightly to the right.\n",
    "* `Senior Citizen`: About 16% of customers are senior citizens.\n",
    "* `Dependents`: More than 70% of customers don't have dependents.\n",
    "* `Phone_Services`: More than 90% of customers have phone services enabled.\n",
    "* `Internet_Service`: 44% of customers use Fibre Optic for internet service. 34% use DSL, while the rest don't have internet services at all. \n",
    "* `Contract`: There are 55% customers with month-to-month contracts. Other two types of contract are: One-year and Two-year\n",
    "* `Payment_Method`: Electronic check is the most used payment method among the four methods of payment.\n",
    "* `Churn`: The churn rate in the data is about 26%.\n",
    "* `Churn vs Senior_Citizen`: Among Senior Citizen customers, the churn rate is about 41%. Senior Citizens are more likely to churn compare to others.\n",
    "* `Churn vs Internet_Service`: Among customers who don't use Internet Service, the churn rate is very low(8%). While, the churn rate is highest for Fibre Optic users(42%).\n",
    "* `Churn vs Contract`: As the length of contract increases, the likelihood of churning decreases. 43% of monthly contract customers are likely to churn, followed by 11% of one-year contracts, while two-year contract customers have the least churn rate of 3%\n",
    "* `Churn vs Payment_Method`: Customers with Electronic Check payment have a higher churn rate than any other payment method.\n",
    "* `Churn vs Tenure`: As tenure increases, the customers are less likely to churn. Customers with low tenure have churned the most.\n",
    "* `Churn vs Charges`: Customers who have churned, have higher monthly charges but lower total charges.\n",
    "* `Contract vs Internet_Service`: Among the month-to-month contract customers, the most used service for Internet is Fiber Optic. Among the one-year and two-year contract customers, DSL service is more used as compared to Fiber Optic.\n",
    "* `Contract vs Payment_Method`: Among the month-to-month contract customers, Electronic check method of payment is used extensively. Among the one-year and two-year contract customers, Credit Card and Bank transfer methods of payment are more used as compared to other methods\n",
    "* `Internet_Service vs Payment_Method`: Customers without internet service use the mailed check payment method the most. Customers with Fibre Optice internet service use the Electronic Check method the most.\n",
    "\n",
    "* In many other columns, like Online_Security, Online_Backup, Tech_Support, Streaming_Movies, etc. there is a level named 'No internet service'. Moreover, the count for the 'No internet service' level is also the same in all columns. This means that customers with No internet service don't have access to many other services like online security, streaming movies, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* K-Nearest Neighbors and Random Forest models overfit the data and is not able to generalise well.\n",
    "* The accuracies of Logistic Regression, Decision Tree and XGBoost models perform well in both training and test dataset.\n",
    "* Logistic Regression have given a generalised performance with high recall and high precision.\n",
    "* Overall, let's see what the precision and recall means in customer churn-\n",
    "\n",
    "**Precision** - Of all the customers that the algorithm predicts will churn, how many of them do actually churn?\n",
    "\n",
    "**Recall** – What percentage of customers that end up churning does the algorithm successfully find?\n",
    "\n",
    "Both precision and recall values are important for customer churn.\n",
    "\n",
    "High recall and low precision means the model is unnecessarily predicting non-churned customers as churned, adding overhead to the business.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The company should attract the customers likely to churn with bonus plans or discounts on recharges.\n",
    "* The company should improve Fiber optic internet services as its use is somehow increasing the probability of churn\n",
    "* The company should try to attract the newly joined customers with attractive offers so that they stay longer with the network. And also, try to make long-term contracts with the customers.\n",
    "* As per observations from the model, customers who use the internet to stream TV and movies have higher chances of churning than those who don't. This may be due to the poor internet connection faced by the customer. The company should improve the internet connectivity to check this.\n",
    "* Online Security and Tech Support should be provided to as many customers as possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('Diabetes-Classification-Ri9St7y4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6c85e3b5405eddcb28f14222d137248a0efd477ee81d42ce12f26367ca419b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
